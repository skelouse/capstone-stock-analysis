{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Predicting-Stock-Data-with-an-LSTM-Network\" data-toc-modified-id=\"Predicting-Stock-Data-with-an-LSTM-Network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Predicting Stock Data with an LSTM Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#This-repository-contains\" data-toc-modified-id=\"This-repository-contains-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>This repository contains</a></span></li><li><span><a href=\"#Using-the-OSEMN-Process\" data-toc-modified-id=\"Using-the-OSEMN-Process-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Using the OSEMN Process</a></span></li><li><span><a href=\"#Scrubbing-the-data\" data-toc-modified-id=\"Scrubbing-the-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Scrubbing the data</a></span></li><li><span><a href=\"#Visualzations\" data-toc-modified-id=\"Visualzations-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Visualzations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-AAPL-price-to-yahoo-finance\" data-toc-modified-id=\"Comparing-AAPL-price-to-yahoo-finance-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Comparing AAPL price to yahoo finance</a></span></li></ul></li><li><span><a href=\"#Next-Steps:\" data-toc-modified-id=\"Next-Steps:-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Next Steps:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Stock Data with an LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This repository contains\n",
    "\n",
    "- A `main.ipynb` Jupyter notebook detailing EDA and the prediction process\n",
    "\n",
    "\n",
    "- Folder `db` with files `firebase.py` and `database.py` for connecting to SQL and Google Firebase\n",
    "\n",
    "\n",
    "- Folder `models` with files\n",
    "  - `script.py` for using keras tuner to find the best parameters for a network such as \"n_hidden_layers\", \"batch_size\" and regularizer parameters\n",
    "  - `create.py` for creating a network with given parameters, and testing parameters.\n",
    "  - `load.py` for loading in a network with the best parameters and making predictions.\n",
    "  \n",
    "\n",
    " - Folder `old` with files\n",
    "  - `Old main.ipynb` which is my original notebook with all of the scrubbing process included\n",
    "  - `Old model_creation.ipynb` (unorganized) where I first started making my Time Series network, and testing different ways to do so.\n",
    "  - `Old Modeling.ipynb` (unorganized) Where I tested more aspects of making the network, separated out from Old main\n",
    "  \n",
    "  \n",
    " - Folder `predictions` where all of the predictions are stored for future visualization of how the network is doing.\n",
    " \n",
    " \n",
    " - `styles/custom.css`  The css used to style the jupyter notebooks\n",
    " \n",
    " \n",
    " - Folder `test` with files\n",
    "  - `dashboard_test.ipynb` with my first tests of plotly graphs and my scraped data for my <a href=\"https://sql-viewer.herokuapp.com/\">Website</a>\n",
    "  - `Firebase Test.ipynb` for testing making plotly graphs directly from Firebase API calls\n",
    "  - `model_scratch_testing.ipynb` Another model testing function that the NetworkCreator class was built off of.\n",
    "  \n",
    "  \n",
    " - `Prediction_testing.ipynb` A notebook for testing making predictions for tomorrow,  later combined into `main.ipynb` for multi-day prediction.\n",
    " \n",
    " \n",
    " - `Pull and clean data.ipynb`  My original notebook for pulling and replacing all of the clean data pickles, and cleaned data in the 'stock_cleaned' SQL server.\n",
    " \n",
    " \n",
    " - `Pull and update data.ipynb` A later rendition of pull and clean that simply cleans and inserts the new data rather than all of the data.  Takes 5 minutes to run vs almost an hour previously.\n",
    " \n",
    " \n",
    " - `presentation.pdf` A technical presentation of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OSEMN Process\n",
    "- Obtain the data\n",
    "- Scrub the data\n",
    "- Explore the data\n",
    "- Model the data\n",
    "- Interpret the data\n",
    "- <a href=\"https://machinelearningmastery.com/how-to-work-through-a-problem-like-a-data-scientist/\">Reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrubbing the data\n",
    "\n",
    "<div class=\"alert alert-info shadow\">\n",
    "  <strong>Prices</strong>\n",
    "    <ul>\n",
    "        <li>Reindex to valid dates 2019-08-09 => onwards without the four days that have very little data '2019-12-09', '2020-06-23', '2020-06-24', '2020-06-25'</li>\n",
    "        <li>Forward interpolate the data with a limit of three days.  So if 6-25 was a valid price, and the four days after were null it would fill the first three, but not the third</li>\n",
    "        <li>Drop symbols with null values</li>\n",
    "        <li>Post to <code>stock_cleaned</code> SQL server</li>\n",
    "        <li>Pickle</li>\n",
    "    </ul>\n",
    "    <br></br>\n",
    "    <strong>Splits</strong>\n",
    "    <ul>\n",
    "        <li>Make an apply column which is `num`/`den`</li>\n",
    "    </ul>\n",
    "    <br></br>\n",
    "    <strong>Performance</strong>\n",
    "    <ul>\n",
    "        <li>Load in performance and clean</li>\n",
    "        <li>Drop symbols not in price symbols</li>\n",
    "        <li>Match index to price index</li>\n",
    "        <li>Fill null ExDividend dates with 1970-01-01 then encode days since then for numerical data</li>\n",
    "        <li>Decide columns to fill, and columns to fill then drop if the symbol still has null values</li>\n",
    "        <li>Interpolate null values for both, fill na for columns to fill</li>\n",
    "        <li>Drop columns with negative min that still have many null values</li>\n",
    "        <li>Drop symbols that still have null values in the columns with a negative minimum as filling with 0 not be adequate.</li>\n",
    "        <li>Add price to performance</li>\n",
    "        <li>Apply splits</li>\n",
    "        <li>Separate out penny stocks ( stocks where price is < 1 dollar )</li>\n",
    "        <li>Post to <code>stock_cleaned</code> SQL server</li>\n",
    "        <li>Pickle penny and non-penny performances</li>\n",
    "    </ul>\n",
    "    <br></br>\n",
    "    <strong>Company</strong>\n",
    "    <ul>\n",
    "        <li>Split out symbols that are in performance symbols</li>\n",
    "        <li>Fill null text values with `unknown`</li>\n",
    "        <li>Pickle.</li>\n",
    "    </ul>\n",
    "    <br></br>\n",
    "    <strong>Analyst</strong>\n",
    "    <ul>\n",
    "        <li>Front fill null values by symbol, then fill the rest with 0</li>\n",
    "        <li>Map text values to numeric</li>\n",
    "        <li>Convert all to float</li>\n",
    "        <li>Post to <code>stock_cleaned</code> SQL server</li>\n",
    "        <li>Pickle</li>\n",
    "    </ul>\n",
    "    <br></br>\n",
    "    <strong>Combined Company/Analyst/Performance</strong>\n",
    "    <ul>\n",
    "        <li>One hot encode Company</li>\n",
    "        <li>Combine the three dataframes into one</li>\n",
    "    </ul>\n",
    "    <p><b>After process is done update Firebase for website with performance and performance penny,  possibly company and analyst if added later</b></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualzations\n",
    "\n",
    "### Comparing AAPL price to yahoo finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "> * Cluster on absolute correlation?  take.corr for different symbols\n",
    "> * iteratively add columns,  start with only predicting future price of one stock with all the prices previously of other stocks\n",
    "> * Try with vs without differencing and/or scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
