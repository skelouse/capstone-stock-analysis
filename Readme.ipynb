{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Predicting-Stock-Data-with-an-LSTM-Network\" data-toc-modified-id=\"Predicting-Stock-Data-with-an-LSTM-Network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Predicting Stock Data with an LSTM Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Abstract\" data-toc-modified-id=\"Abstract-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Abstract</a></span></li><li><span><a href=\"#This-repository-contains\" data-toc-modified-id=\"This-repository-contains-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>This repository contains</a></span></li><li><span><a href=\"#Using-the-OSEMN-Process\" data-toc-modified-id=\"Using-the-OSEMN-Process-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Using the OSEMN Process</a></span></li></ul></li><li><span><a href=\"#Scrubbing-the-data\" data-toc-modified-id=\"Scrubbing-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scrubbing the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prices\" data-toc-modified-id=\"Prices-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Prices</a></span></li><li><span><a href=\"#Splits\" data-toc-modified-id=\"Splits-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Splits</a></span></li><li><span><a href=\"#Performance\" data-toc-modified-id=\"Performance-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Performance</a></span></li><li><span><a href=\"#Company\" data-toc-modified-id=\"Company-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Company</a></span></li><li><span><a href=\"#Analyst\" data-toc-modified-id=\"Analyst-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Analyst</a></span></li><li><span><a href=\"#Combined-Company/Analyst/Performance\" data-toc-modified-id=\"Combined-Company/Analyst/Performance-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Combined Company/Analyst/Performance</a></span></li></ul></li><li><span><a href=\"#Visualzations\" data-toc-modified-id=\"Visualzations-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Visualzations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Our-SP500\" data-toc-modified-id=\"Our-SP500-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Our SP500</a></span></li><li><span><a href=\"#Comparing-our-scraped-AAPL-price-to-yahoo-finance\" data-toc-modified-id=\"Comparing-our-scraped-AAPL-price-to-yahoo-finance-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Comparing our scraped AAPL price to yahoo finance</a></span></li><li><span><a href=\"#AAPL-differencing-of-features-compared-to-price\" data-toc-modified-id=\"AAPL-differencing-of-features-compared-to-price-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>AAPL differencing of features compared to price</a></span></li><li><span><a href=\"#Positive-and-negative-correlations-to-price\" data-toc-modified-id=\"Positive-and-negative-correlations-to-price-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Positive and negative correlations to price</a></span></li><li><span><a href=\"#Lack-of-overall-correlations\" data-toc-modified-id=\"Lack-of-overall-correlations-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Lack of overall correlations</a></span></li></ul></li><li><span><a href=\"#First-model\" data-toc-modified-id=\"First-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>First model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Base-model-predictions\" data-toc-modified-id=\"Base-model-predictions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Base model predictions</a></span></li><li><span><a href=\"#Manually-tuned-model-predictions\" data-toc-modified-id=\"Manually-tuned-model-predictions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Manually tuned model predictions</a></span></li><li><span><a href=\"#Auto-tuned-model-predictions\" data-toc-modified-id=\"Auto-tuned-model-predictions-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Auto-tuned model predictions</a></span></li></ul></li><li><span><a href=\"#Hyper-parameter-tuning-methodology\" data-toc-modified-id=\"Hyper-parameter-tuning-methodology-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Hyper-parameter tuning methodology</a></span><ul class=\"toc-item\"><li><span><a href=\"#tune.py\" data-toc-modified-id=\"tune.py-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>tune.py</a></span></li><li><span><a href=\"#nt.tune\" data-toc-modified-id=\"nt.tune-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>nt.tune</a></span></li><li><span><a href=\"#NetworkBuilder\" data-toc-modified-id=\"NetworkBuilder-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>NetworkBuilder</a></span></li><li><span><a href=\"#tuner.search\" data-toc-modified-id=\"tuner.search-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>tuner.search</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Next-Steps:\" data-toc-modified-id=\"Next-Steps:-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Next Steps:</a></span></li></ul></li><li><span><a href=\"#Folder-Structure\" data-toc-modified-id=\"Folder-Structure-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Folder Structure</a></span></li><li><span><a href=\"#Repository-Structure\" data-toc-modified-id=\"Repository-Structure-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Repository Structure</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Stock Data with an LSTM Network\n",
    "\n",
    "**Author**: <a href=\"https://sites.google.com/skelouse.com/home/\">Sam Stoltenberg</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body>\n",
    "<p>\n",
    "Do you want to GET RICH QUICK?  Going this route has many heartaches, and decsision trees to traverse such as.  Drop some sparse columns, or fill the missing data with 0?  Do you one hot encode an analyst saying \"BUY\", \"HOLD\", or map numerical values to each of the given features? What if there are irregularties in the price data you have, how do you fix them?  All of these and more are valid questions we had to answer, and have a direct effect on the predictions of our networks.  If in the end our predictions do gain profits we will become market movers, and eventually the market will work out the methods we are using to predict.</p>\n",
    "<p>\n",
    "Our data was scraped using Selenium from an investment firm consisting of analyst opinions performance statistics, prices, and company information for 7000+ stock symbols from August, 9th of 2019 to present.  Although after cleaning our data, and dropping irregularties we end with roughly 2000 symbols.</p>\n",
    "\n",
    "<p>We are predicting time series data, so we have to define things such as the number of days to predict the next with.  The data is then transformed into multiple matrices of X_data correlating to y_targets.  The X_data being all of the data from n day(s) before, and the y_targets being the data we are trying to predict. If one wanted to know the information two days ahead they would have to predict all of the data for one day then use the predicted data to predict the next, or structure the data in such a way where one day is being used to predict two.</p>\n",
    "    \n",
    "<p>Our first networks had infinite loss due to predicting everything as 0, so we had to devise a method for creating the best network to use on the data.  There are few `plug-and-play` methods for tuning neural networks, and especially tuning Time Series predicting networks.  The method we did find was a Hyperband from `LINK-kerastuner`.  The Hyperband takes a build function and inside of the build function one can use an Hyperband choice function which reports back to the Hyperband what effect a given quotient had on the validation loss of the network.  After refactoring and development the tuner can now tune items such as:\n",
    "<ul>\n",
    "    <li>n_input (number of days to use in the prediction)</li>\n",
    "    <li>columns (which of the given columns to use in the prediction)</li>\n",
    "    <li>scale or not to scale the data between 0 and 1</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "After developing with the Hyperband we developed a cross validation method, as kerastuner does not supply one out of the bag for time series.  Cross validation ensures that the parameters are not being tuned solely for one set of testing data.  K validation sets are also held back throughout the tuning process to test the network on at the end of tuning.</p>\n",
    "</body\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This repository contains\n",
    "\n",
    " -  A Jupyter notebook <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/main.ipynb\">`main.ipynb`</a> detailing my EDA and neural network processes\n",
    " \n",
    " \n",
    " A technical <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/presentation.pdf\">`presentation.pdf`</a> of the project.\n",
    " \n",
    " \n",
    "  A python script <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/tune.py\">`tune.py`</a> which is ran with:\n",
    "```\n",
    "python tune.py [name_of_network]\n",
    "# name_of_network being a predefined name correlating to a function that tunes that specific network on predefined hyper-parameters\n",
    "```\n",
    "\n",
    " -  A Jupyter notebook <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/Pull%20and%20clean%20data.ipynb\">`Pull and clean data.ipynb`</a> for pulling and replacing all of the clean data pickles, refreshing data in the 'stock_cleaned' SQL server, and refreshing the Firebase database. \n",
    "\n",
    "\n",
    "- Folder <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/db\">`db`</a> with files <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/db/firebase.py\">`firebase.py`</a> and <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/db/database.py\">`database.py`</a> for connecting to and posting to Google Firebase and our SQL server.\n",
    "\n",
    "\n",
    "- Folder <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/modeling\">`modeling`</a> with files:\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/modeling/build.py\">`build.py`</a> \n",
    "with class NetworkBuilder which takes parameters that directly correlate to how a network is put together.  This class is also used in the tuner, for tuning those same parameters.\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/modeling/create.py\">`create.py`</a> \n",
    "with class NetworkCreator that does everything from preparing the time series data to creating an html report on how well the model performed on the train, test, and validation data.\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/modeling/sequential.py\">`sequential.py`</a> \n",
    "with class CustomSequential for wrapping a keras Sequential model and overriding it's fit function to implement a custom cross validation method.\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/modeling/tuner.py\">`tuner.py`</a> \n",
    "with class NetworkTuner for tuning a neural network's architecture, and data processing methods.\n",
    "\n",
    "\n",
    "- Folder <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/old\">`old`</a> (unorganized) with files:\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/old/Old%20Modeling.ipynb\">`Old Modeling.ipynb`</a> \n",
    "Which is a Jupyter Notebook where I failed to predict on all of the data\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/old/Old%20main.ipynb\">`Old main.ipynb`</a> \n",
    "Which is my original Jupyter Notebook containing the scrubbing process, and attempts at modeling\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/old/Old%20main2.ipynb\">`Old main2.ipynb`</a> \n",
    "Which is a Jupyter Notebook showing my attempt at predicting all of the data from three sides before realizing it was basically impossible, and that company info is irrelevant there since it is unchanging.\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/old/Pull%20and%20update%20data.ipynb\">`Pull and update data.ipynb`</a> \n",
    "Which is an almost working notebook for updating the data rather than pulling it all and updating everything.\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/old/scratch.ipynb\">`scratch.ipynb`</a> \n",
    "Which is a Jupyter Notebook showcasing where I really dug into Time Series data, exactly what the generator was doing, and forecasting.\n",
    "\n",
    "- Folder <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/reports\">`reports`</a> containing HTML reports of how each model performed, and which columns directly effected their performance.\n",
    "\n",
    "- File <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/styles/custom.css\">`styles/custom.css`</a> containing the css used to style the jupyter notebooks\n",
    " \n",
    " \n",
    " - Folder <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/test_notebooks\">`test_notebooks`</a> (unorganized) with files:\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/test_notebooks/Firebase%20Test.ipynb\">`Firebase Test.ipynb`</a> \n",
    "Which is a Jupyter Notebook\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/test_notebooks/Prediction_testing.ipynb\">`Prediction_testing.ipynb`</a> \n",
    "Which is a Jupyter Notebook testing predictions with my old method of greek god named models.\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/test_notebooks/dashboard_test.ipynb\">`dashboard_test.ipynb`</a> \n",
    "Which is a Jupyter Notebook with my first tests of plotly graphs and my scraped data for my <a href=\"https://sql-viewer.herokuapp.com/\">Website</a>\n",
    "\n",
    "  - <a href=\"https://github.com/skelouse/capstone-stock-analysis/blob/master/test_notebooks/model_scratch_testing.ipynb\">`model_scratch_testing.ipynb`</a> \n",
    "Which is a Jupyter Notebook containing the actual function tests that were used in the beginning development of my NetworkCreator class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OSEMN Process\n",
    "- Obtain the data\n",
    "- Scrub the data\n",
    "- Explore the data\n",
    "- Model the data\n",
    "- Interpret the data\n",
    "- <a href=\"https://machinelearningmastery.com/how-to-work-through-a-problem-like-a-data-scientist/\">Reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing the data\n",
    "  <h2>Prices</h2>\n",
    "    <ul>\n",
    "        <li>Reindex to valid dates 2019-08-09 => onwards without the four days that have very little data '2019-12-09', '2020-06-23', '2020-06-24', '2020-06-25'</li>\n",
    "        <li>Forward interpolate the data with a limit of three days.  So if 6-25 was a valid price, and the four days after were null it would fill the first three, but not the third</li>\n",
    "        <li>Drop symbols with null values</li>\n",
    "        <li>Post to <code>stock_cleaned</code> SQL server</li>\n",
    "        <li>Pickle</li>\n",
    "    </ul>\n",
    "    <h2>Splits</h2>\n",
    "    <ul>\n",
    "        <li>Make an apply column which is `num`/`den`</li>\n",
    "    </ul>\n",
    "    <h2>Performance</h2>\n",
    "    <ul>\n",
    "        <li>Load in performance and clean</li>\n",
    "        <li>Drop symbols not in price symbols</li>\n",
    "        <li>Match index to price index</li>\n",
    "        <li>Fill null ExDividend dates with 1970-01-01 then encode days since then for numerical data</li>\n",
    "        <li>Decide columns to fill, and columns to fill then drop if the symbol still has null values</li>\n",
    "        <li>Interpolate null values for both, fill na for columns to fill</li>\n",
    "        <li>Drop columns with negative min that still have many null values</li>\n",
    "        <li>Drop symbols that still have null values in the columns with a negative minimum as filling with 0 not be adequate.</li>\n",
    "        <li>Add price to performance</li>\n",
    "        <li>Apply splits</li>\n",
    "        <li>Separate out penny stocks ( stocks where price is < 1 dollar )</li>\n",
    "        <li>Post to <code>stock_cleaned</code> SQL server</li>\n",
    "        <li>Pickle penny and non-penny performances</li>\n",
    "    </ul>\n",
    "    <h2>Company</h2>\n",
    "    <ul>\n",
    "        <li>Split out symbols that are in performance symbols</li>\n",
    "        <li>Fill null text values with `unknown`</li>\n",
    "        <li>Pickle.</li>\n",
    "    </ul>\n",
    "    <h2>Analyst</h2>\n",
    "    <ul>\n",
    "        <li>Interpolate null values by symbol, then fill the rest with 0</li>\n",
    "        <li>Map text values to numeric</li>\n",
    "        <li>Convert all to float</li>\n",
    "        <li>Post to <code>stock_cleaned</code> SQL server</li>\n",
    "        <li>Pickle</li>\n",
    "    </ul>\n",
    "    <h2>Combined Company/Analyst/Performance</h2>\n",
    "    <ul>\n",
    "        <li>One hot encode Company</li>\n",
    "        <li>Combine the three dataframes into one</li>\n",
    "    </ul>\n",
    "    <p><b>After the process is complte, we update Firebase for website with performance and performance penny,  possibly company and analyst if added later</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualzations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our SP500\n",
    "> This is an average of our SP500 prices you can clearly see the covid-19 dip in march"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sp500](./img/sp500.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our scraped AAPL price to yahoo finance\n",
    "> As you can see below, our data is not perfect as it is only collected once per day, but we have many more features then we know what to do with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compared_sp500](./img/our_data_compared.png)\n",
    "©2020 Verizon Media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAPL differencing of features compared to price\n",
    "> Here we differenced each feature so that the 2nd day is now (1st day) subtracted from (2nd day) and so on.  We than plot that on the same scale as price to see if there are any indicators of price jumps, and to check the vitality of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:45:58.359534Z",
     "start_time": "2020-12-20T12:45:58.302358Z"
    }
   },
   "source": [
    "![aapl_differenced](./img/aapl_differ.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>About the plot:</strong>\n",
    "    <ul>\n",
    "        <li>You can see when appl price split in `VolumeAvg90Day` peaked.\n",
    "        </li>\n",
    "        <li>Features such as `ReturnonEquity` are quarterly reports, thus they are showing differencing on the quarters.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive and negative correlations to price\n",
    "> Pay special attention to the columns in red and green as they are showing negative and positive correlation to price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![positive_negative_price](./bin/price_correlation_difference_one_day.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>About the plot:</strong>\n",
    "        You can see for VLO( Valero ) <span style=\"color:red\">`PE`, in red</span> has a negative correlation to price, and a positive correlation to price <span style=\"color:green\">in green , `PriceToSales`</span>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of overall correlations\n",
    "> Here we took the overall prediction quality of each column, and plotted their sum qualities for each symbol.\n",
    "> The quality was determined by how well a given feature correlated to changes in all of the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lack_of_correlation](./bin/highlight_bad_columns_one_day.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T12:55:33.659443Z",
     "start_time": "2020-12-20T12:55:33.649449Z"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>About the plot:</strong>\n",
    "        In red you can see the columns that we dropped.  They are overall more normally distributed, thus not useful in predicting what tomorrow's price is going to be\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model predictions\n",
    "![base_pred](./bin/base_model_pred.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T13:52:37.028763Z",
     "start_time": "2020-12-20T13:52:37.010751Z"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>About the plot!</strong>\n",
    "    <ul>\n",
    "        <li>The network is not doing to well at predicting the test or validation data</li>\n",
    "        <li>Drop in quality of the testing data is showing through, as AAPL had a split in September of 2020</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually tuned model predictions\n",
    "> After some slight manual tuning of the network here are the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_pred](./bin/tuned_pred.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>About the plot!</strong>\n",
    "    <ul>\n",
    "        <li>Much better than the base model</li>\n",
    "        <li>You can see the same split drop in quality here on the testing data, maybe we could remove outliers.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-tuned model predictions\n",
    "> Coming soon..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning methodology\n",
    "> Here we will do a simple walk through of the Hyper-parameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T13:21:39.186832Z",
     "start_time": "2020-12-20T13:21:39.174836Z"
    }
   },
   "source": [
    "## tune.py\n",
    "> Here is a small test for the tune.py you can see that there are many different parameters defined.  `input_neurons` corresponds to how many neurons will be used in the input layer.  `n_days` corresponds to how many days are used for predicting the next, or the length of the TimeSeriesGenerator.\n",
    "```python\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# Import our NetworkTuner\n",
    "from modeling.tuner import NetworkTuner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define parameters to tune\n",
    "    parameters = {\n",
    "        'input_neurons': [2, 4, 8, 16],\n",
    "        'input_dropout_rate': [.1, .3, .5],\n",
    "        'use_input_regularizer': [0, 1, 2],\n",
    "        'input_regularizer_penalty': [0.01, 0.05, 0.1, 0.3],\n",
    "        'n_hidden_layers': [1, 3, 5, 8],\n",
    "        'hidden_dropout_rate': [0.0, .3, .5, .9],\n",
    "        'hidden_neurons': [16, 32, 64],\n",
    "        'use_hidden_regularizer': [0, 1, 2],\n",
    "        'hidden_regularizer_penalty': [0.01, 0.05, 0.1, 0.3],\n",
    "        'patience': [5, 25, 50, 100],\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'use_early_stopping': [0, 1],\n",
    "        'n_days': [1, 2, 3]\n",
    "    }\n",
    "    \n",
    "    # Build the test dataframe\n",
    "    _list = list(range(20))\n",
    "    df = pd.DataFrame({\n",
    "        'apple': copy.copy(_list),\n",
    "        'orange': copy.copy(_list),\n",
    "        'banana': copy.copy(_list),\n",
    "        'pear': copy.copy(_list),\n",
    "        'cucumber': copy.copy(_list),\n",
    "        'tomato': copy.copy(_list),\n",
    "        'plum': copy.copy(_list),\n",
    "        'watermelon': copy.copy(_list)\n",
    "    })\n",
    "    \n",
    "    # Define which columns are feature(s) and which are the target(s)\n",
    "    X_cols = list(df.columns)\n",
    "    y_cols = 'banana'\n",
    "\n",
    "\n",
    "\"On the instantiation of NetworkTuner our data is split into k many folds, and then each fold is split again into training, testing, and validation data.\"\n",
    "\n",
    "    # Instantiate our NetworkTuner\n",
    "    nt = NetworkTuner(\n",
    "        df=df, X_cols=X_cols,\n",
    "        y_cols=y_cols, k_folds=5, max_n_days=3\n",
    "    )\n",
    "    \n",
    "    # Call the tune function\n",
    "    nt.tune(\n",
    "        'Albert', max_epochs=100\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nt.tune\n",
    "> When nt.tune is ran the following function is called from modeling.NetworkTuner\n",
    "\n",
    "```python\n",
    "def tune(self, name, max_epochs=10, **parameters):\n",
    "    \"\"\"Running the tuner with kerastuner.Hyperband\"\"\"\n",
    "\n",
    "    # Feeding parameters to tune into the build function\n",
    "    # before feeding it into the Hyperband\n",
    "    self.build_and_fit_model = partial(\n",
    "        self.build_and_fit_model, **parameters\n",
    "    )\n",
    "\n",
    "    # Register Logger dir and instantiate kt.Hyperband\n",
    "    Logger.register_directory(name)\n",
    "    tuner = kt.Hyperband(self.build_and_fit_model,\n",
    "                            objective='val_loss',\n",
    "                            max_epochs=max_epochs,\n",
    "                            factor=3,\n",
    "                            directory='./tuner_directory',\n",
    "                            project_name=name,\n",
    "                            logger=Logger)\n",
    "\n",
    "    # Start the search for best hyper-parameters\n",
    "    tuner.search(self)\n",
    "\n",
    "    # Get the best hyper-parameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # Display the best hyper-parameters\n",
    "    print(f\"\"\"The hyperparameter search is complete.\n",
    "    The optimal number of units in the first densely-connected layer\n",
    "    {best_hps.__dict__['values']}\n",
    "    \"\"\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetworkBuilder\n",
    "> The NetworkBuilder has a series of functions like the below for actually searching the different parameters, getting each selection from the Hyperband.  Here is a small cut-out of our input layer showcasing where the Hyperband makes choices.\n",
    "\n",
    "```python\n",
    "input_neurons = hp.Choice('input_neurons', input_neurons)\n",
    "model.add(LSTM(input_neurons))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tuner.search\n",
    "> As the tuner begins its' search we move to our CustomSequential that is used by the NetworkTuner as the its' primary network when tuning.  The CustomSequential overrides tensorflow.keras.models.Sequantial fit function to implement a cross-validation split.  A simplified version of our CustomSequential.fit is defined as follows:\n",
    "\n",
    "```python\n",
    "def fit(self, nt, **kwargs):\n",
    "    \"\"\"\n",
    "    Overrides model fit to call it k_folds times\n",
    "    then averages the loss and val_loss to return back\n",
    "    as the history.\n",
    "    \"\"\"\n",
    "\n",
    "    histories = []\n",
    "    h = None\n",
    "\n",
    "    # Iterate over number of k_folds\n",
    "    for k in range(1, self.k_folds+1):\n",
    "        train, test, val = self.nt.n_day_gens[self.n_days][k]\n",
    "        # Split data and targets\n",
    "        X, y = train[0]\n",
    "        X_t, y_t = test[0]\n",
    "\n",
    "        # Calling Sequential.fit() with each fold\n",
    "        h = super(CustomSequential, self).fit(\n",
    "            X, y,\n",
    "            validation_data=(X_t, y_t),\n",
    "            **kwargs)\n",
    "        histories.append(h.history)\n",
    "\n",
    "    # Get and return average of model histories\n",
    "    df = pd.DataFrame(histories)\n",
    "    h.history['loss'] = np.array(df['loss'].sum()) / len(df)\n",
    "    h.history['val_loss'] = np.array(df['val_loss'].sum()) / len(df)\n",
    "    return h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "* Data can be manipulated in many different ways, and networks can be tuned in many different ways.  To accurately predict the stock market one would have to come across a lucky set of hyper-parameters, and training set that the big players have not tried on their huge servers.  The parameters chosen would also not work forever.\n",
    "\n",
    "* Over time if you are trading in large volumes the market would become \"used\" to your predictions, and the market movers would start basing their predictions off of yours, and they would become useless.\n",
    "\n",
    "* Coming soon..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "> * Cluster on absolute correlation?  take.corr for different symbols\n",
    "> * Tune network on which columns are being used for predictions\n",
    "> * Tune network with vs without differencing and/or scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Structure\n",
    "```\n",
    "\\--- bin\n",
    "|\n",
    "\\--- db\n",
    "|\n",
    "\\--- img\n",
    "|\n",
    "\\--- modeling\n",
    "\\--- \\--- tests\n",
    "\\         \\--- _python\n",
    "\\         \\--- create\n",
    "\\         \\--- tuner\n",
    "|   \n",
    "\\--- old\n",
    "|\n",
    "\\--- reports\n",
    "\\    \\--- aapl_price_w_aapl_info\n",
    "\\    \\--- aapl_price_w_all_price\n",
    "\\    \\--- aapl_price_w_sector\n",
    "|\n",
    "\\--- styles\n",
    "|\n",
    "\\--- test_notebooks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repository Structure\n",
    "\n",
    "```\n",
    "\\--- bin\n",
    "\\    |  __init__.py\n",
    "\\    |  anomoly.py\n",
    "\\    |  database-schema.py\n",
    "\\    |  NN.py\n",
    "\\    |  out.png\n",
    "\\    |  correlation data csv files\n",
    "|\n",
    "|\n",
    "\\--- db\n",
    "\\    |  __init__.py\n",
    "\\    |  database.py\n",
    "\\    |  firebase.py\n",
    "|\n",
    "|\n",
    "\\--- img\n",
    "\\    |  flow.png\n",
    "|\n",
    "|\n",
    "\\--- modeling\n",
    "\\--- \\--- tests\n",
    "\\         \\--- _python\n",
    "\\              |  test_param_setting.py\n",
    "\\\n",
    "\\         \\--- create\n",
    "\\         \\--- tuner\n",
    "\\              |  test_cv.py\n",
    "\\              \\-  val_folds\n",
    "\\    |  __init__.py\n",
    "\\    |  build.py\n",
    "\\    |  create.py\n",
    "\\    |  sequential.py\n",
    "\\    |  tuner.py\n",
    "|   \n",
    "|\n",
    "\\--- old   \n",
    "\\    |  Old main.ipynb\n",
    "\\    |  Old main2.ipynb\n",
    "\\    |  Old model_creation.ipynb\n",
    "\\    |  Old Modeling.ipynb\n",
    "\\    |  Pull and update data.ipynb\n",
    "\\    |  scratch.ipynb\n",
    "\\    |  scratch.py\n",
    "|\n",
    "|\n",
    "\\--- reports\n",
    "\\    \\--- aapl_price_w_aapl_info\n",
    "\\    \\--- aapl_price_w_all_price\n",
    "\\    \\--- aapl_price_w_sector\n",
    "|\n",
    "|\n",
    "\\--- styles\n",
    "\\   |  custom.css\n",
    "\\   |  \n",
    "|\n",
    "|\n",
    "\\--- test_notebooks\n",
    "\\    |  dashboard_test.ipynb\n",
    "\\    |  Firebase Test.ipynb\n",
    "\\    |  model_scratch_testing.ipynb\n",
    "\\    |  Prediction_testing.ipynb\n",
    "|\n",
    "|  .gitignore\n",
    "|  main.ipynb\n",
    "|  presentation.pdf\n",
    "|  Pull and clean data.ipynb\n",
    "|  Readme.ipynb\n",
    "|  README.md\n",
    "|  run_tests.py\n",
    "|  todo.txt\n",
    "|  tune.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
